# Context Management Strategies - Comparison

## ğŸ¯ The Problem

**Context Overflow Error (at iteration 97):**
```
Error code: 400 - maximum context length is 40730 tokens.
However, your request has 98398 input tokens.
```

AutoGen agents accumulate conversation history over time. With 8000 iterations, this quickly exceeds the model's 40,730 token limit.

---

## ğŸ”€ Three Solutions Compared

### **Strategy 1: Agent Reset** (Original Fix)
**Files:** `run_pde_discovery_simple_v04_fixed.py`, `RUN_SIMPLE_V04_FIXED.sh`

**How it works:**
- Creates a **fresh agent** every 50 iterations
- Completely clears conversation history
- Starts with empty context

**Code Pattern:**
```python
def _create_assistant(self, system_message: str) -> AssistantAgent:
    """Create a fresh AssistantAgent with no history"""
    return AssistantAgent(...)

# Every 50 iterations:
if iteration % reset_interval == 0:
    assistant = self._create_assistant(base_system_message)
```

**Pros:**
- âœ… Simple to implement
- âœ… Guaranteed to prevent context overflow
- âœ… Works with current vLLM (no restart)

**Cons:**
- âŒ **Loses ALL conversation history**
- âŒ **Agent forgets recent learning**
- âŒ No continuity between reset points
- âŒ May rediscover same equations

---

### **Strategy 2: Sliding Window** (Recommended)
**Files:** `run_pde_discovery_simple_v04_sliding.py`, `RUN_SIMPLE_V04_SLIDING.sh`

**How it works:**
- Maintains a **sliding window** of recent messages
- Keeps last N messages (default: 20)
- Trims old messages periodically (every 10 iterations)
- Preserves recent learning and context

**Code Pattern:**
```python
self.conversation_history: List[ChatMessage] = []

def _trim_conversation_history(self):
    """Keep only the most recent N messages"""
    if len(self.conversation_history) > self.context_window_size:
        # Keep only last 20 messages
        self.conversation_history = self.conversation_history[-20:]

# Every 10 iterations:
if iteration % context_trim_interval == 0:
    self._trim_conversation_history()

# Send messages with history:
messages_to_send = list(self.conversation_history)
messages_to_send.append(TextMessage(content=prompt, source="user"))
response = await assistant.on_messages(messages_to_send, cancellation_token)

# Update history:
self.conversation_history.append(TextMessage(content=prompt, source="user"))
self.conversation_history.append(response.chat_message)
```

**Pros:**
- âœ… **Preserves recent learning**
- âœ… **Maintains conversation continuity**
- âœ… Agent remembers recent discoveries
- âœ… Better guidance from recent context
- âœ… Prevents context overflow
- âœ… Works with current vLLM (no restart)

**Cons:**
- âš ï¸ Slightly more complex implementation
- âš ï¸ Requires manual history management

---

### **Strategy 3: Tool Calling with Sliding Window** (Best, requires vLLM restart)
**Files:** `run_pde_discovery_autogen_v04_sliding.py`, `RUN_AUTOGEN_V04_SLIDING.sh`

**How it works:**
- Same sliding window approach
- Uses AutoGen v0.4 tool calling feature
- Agent calls `evaluate_pde` tool directly
- More structured than text parsing

**Code Pattern:**
```python
# Same sliding window as Strategy 2, plus:
assistant = AssistantAgent(
    name="PDE_Generator",
    model_client=self.model_client,
    tools=[self.evaluate_pde_tool],  # Tool calling
    reflect_on_tool_use=True,
    system_message=system_message,
)
```

**Pros:**
- âœ… **Preserves recent learning** (sliding window)
- âœ… **Structured tool calling**
- âœ… Agent can call evaluate_pde directly
- âœ… More reliable than text parsing
- âœ… Best of both worlds

**Cons:**
- âŒ **Requires vLLM restart** with `--enable-auto-tool-choice --tool-call-parser hermes`
- âš ï¸ May have compatibility issues with some models

---

## ğŸ“Š Feature Comparison Table

| Feature | Agent Reset | Sliding Window | Tool Calling + Sliding |
|---------|-------------|----------------|------------------------|
| **Preserves Learning** | âŒ No | âœ… Yes | âœ… Yes |
| **Context Overflow Fix** | âœ… Yes | âœ… Yes | âœ… Yes |
| **Conversation Continuity** | âŒ No | âœ… Yes | âœ… Yes |
| **Works with Current vLLM** | âœ… Yes | âœ… Yes | âŒ Needs restart |
| **Tool Calling** | âŒ No | âŒ No | âœ… Yes |
| **Text Parsing** | âœ… Regex | âœ… Regex | âŒ Not needed |
| **Implementation Complexity** | Simple | Medium | Medium |
| **Memory Usage** | Low | Low | Low |
| **Learning Quality** | â­â­ | â­â­â­â­ | â­â­â­â­â­ |

---

## ğŸ¯ Which to Use?

### **Use Sliding Window (Strategy 2) if:**
- âœ… You want **immediate start** (no vLLM restart)
- âœ… You want **better learning** than agent reset
- âœ… Text parsing is acceptable
- âœ… **RECOMMENDED for most users**

**Command:**
```bash
./RUN_SIMPLE_V04_SLIDING.sh
```

---

### **Use Tool Calling + Sliding (Strategy 3) if:**
- âœ… You can **restart vLLM**
- âœ… You want **structured tool calling**
- âœ… You want **best possible results**
- âœ… Best for production use

**Command:**
```bash
# First, restart vLLM:
pkill -f "vllm serve"
./engine_with_tools.sh &
sleep 30

# Then run:
./RUN_AUTOGEN_V04_SLIDING.sh
```

---

### **Use Agent Reset (Strategy 1) if:**
- âš ï¸ You need the **simplest possible** solution
- âš ï¸ You don't care about losing conversation context
- âš ï¸ **Not recommended** (sliding window is better)

**Command:**
```bash
./RUN_SIMPLE_V04_FIXED.sh
```

---

## ğŸ”¬ Technical Deep Dive

### Sliding Window Implementation

**Key Insight:** Instead of discarding ALL history (reset) or keeping ALL history (overflow), keep a **sliding window** of recent messages.

**Window Size Considerations:**

| Window Size | Token Usage* | Learning Quality | Risk of Overflow |
|-------------|-------------|------------------|------------------|
| 10 messages | ~5,000 | â­â­ | Very Low |
| 20 messages | ~10,000 | â­â­â­â­ | Low |
| 30 messages | ~15,000 | â­â­â­â­â­ | Medium |
| 50 messages | ~25,000 | â­â­â­â­â­ | High |

*Approximate token usage depends on message length

**Recommended:** `context_window_size=20` (default)

---

### Trim Interval Considerations

**Trim Interval:** How often to remove old messages

| Interval | Overhead | Context Freshness |
|----------|----------|-------------------|
| Every 5 iterations | High | Very Fresh |
| Every 10 iterations | Low | Fresh (recommended) |
| Every 20 iterations | Very Low | Moderate |
| Every 50 iterations | Minimal | May accumulate |

**Recommended:** `context_trim_interval=10` (default)

---

## ğŸ“ˆ Expected Behavior

### Sliding Window Output:

```
[Iter 10] Generated 4 equations | History: 20 msgs
[Iter 20] Generated 4 equations | History: 40 msgs

ğŸ”„ [Iter 30] Trimming conversation history...
   âœ‚ï¸  Trimmed 20 old messages, kept recent 20

[Iter 40] Generated 4 equations | History: 40 msgs

ğŸ”„ [Iter 50] Trimming conversation history...
   âœ‚ï¸  Trimmed 20 old messages, kept recent 20

ğŸ¯ Iter 234: NEW BEST! Score=8.1234, RÂ²=0.8567
   Equation: Î±Â·Î”g - Î²Â·âˆ‡Â·(gâˆ‡(ln S)) + Î³Â·g(1-g/K)
```

### Agent Reset Output:

```
[Iter 10] Generated 4 equations
[Iter 20] Generated 4 equations
...

â™»ï¸  Resetting agent at iteration 50 (clearing context)

[Iter 60] Generated 4 equations
[Iter 70] Generated 4 equations
...

â™»ï¸  Resetting agent at iteration 100 (clearing context)
```

**Notice:** Sliding window shows **continuous history size**, reset shows **periodic resets**.

---

## ğŸ” Monitoring Context Usage

### TensorBoard Metrics:

Both strategies log context metrics to TensorBoard:

- `context/history_size` - Number of messages in conversation
- `context/trimmed_count` - Messages removed in last trim
- `performance/iteration_time` - Time per iteration
- `performance/buffer_size` - Experience buffer size

**View:**
```bash
tensorboard --logdir logs/pde_discovery_simple_v04_sliding_8k/tensorboard --port 6006
```

---

## ğŸ›ï¸ Configuration Options

### Sliding Window Parameters:

```bash
# Conservative (smaller window, more frequent trims)
/home/gaoch/miniconda3/envs/llmsr/bin/python run_pde_discovery_simple_v04_sliding.py \
  --context_window_size 15 \
  --context_trim_interval 5 \
  --max_iterations 8000 \
  --output_dir logs/pde_conservative

# Aggressive (larger window, less frequent trims)
/home/gaoch/miniconda3/envs/llmsr/bin/python run_pde_discovery_simple_v04_sliding.py \
  --context_window_size 30 \
  --context_trim_interval 20 \
  --max_iterations 8000 \
  --output_dir logs/pde_aggressive

# Recommended (balanced)
/home/gaoch/miniconda3/envs/llmsr/bin/python run_pde_discovery_simple_v04_sliding.py \
  --context_window_size 20 \
  --context_trim_interval 10 \
  --max_iterations 8000 \
  --output_dir logs/pde_balanced
```

### Agent Reset Parameters:

```bash
# More aggressive reset
/home/gaoch/miniconda3/envs/llmsr/bin/python run_pde_discovery_simple_v04_fixed.py \
  --reset_interval 30 \
  --max_iterations 8000 \
  --output_dir logs/pde_reset30

# Less aggressive reset
/home/gaoch/miniconda3/envs/llmsr/bin/python run_pde_discovery_simple_v04_fixed.py \
  --reset_interval 100 \
  --max_iterations 8000 \
  --output_dir logs/pde_reset100
```

---

## ğŸ§ª Quick Test (5 minutes)

Test sliding window vs. agent reset:

```bash
# Test sliding window (50 iterations, ~5 min)
/home/gaoch/miniconda3/envs/llmsr/bin/python run_pde_discovery_simple_v04_sliding.py \
  --dataset logs/pde_discovery_complex/complex_chemotaxis_v2.hdf5 \
  --max_iterations 50 \
  --samples_per_prompt 2 \
  --context_window_size 10 \
  --context_trim_interval 5 \
  --output_dir logs/test_sliding

# Test agent reset (50 iterations, ~5 min)
/home/gaoch/miniconda3/envs/llmsr/bin/python run_pde_discovery_simple_v04_fixed.py \
  --dataset logs/pde_discovery_complex/complex_chemotaxis_v2.hdf5 \
  --max_iterations 50 \
  --samples_per_prompt 2 \
  --reset_interval 25 \
  --output_dir logs/test_reset

# Compare results:
cat logs/test_sliding/discovery_results.json | grep best_score
cat logs/test_reset/discovery_results.json | grep best_score
```

---

## ğŸ† Recommendation

**For immediate use (no vLLM restart needed):**

```bash
./RUN_SIMPLE_V04_SLIDING.sh
```

**Why Sliding Window is Better:**
1. âœ… **Preserves learning:** Agent remembers recent discoveries
2. âœ… **Context continuity:** Smooth progression through iterations
3. âœ… **Better results:** Learning from recent context improves discovery
4. âœ… **Same simplicity:** Works with current vLLM setup
5. âœ… **Prevents overflow:** Just as effective as reset

**When to use Agent Reset:**
- Only if you need the absolute simplest code
- Not recommended for production use

**When to use Tool Calling + Sliding:**
- If you can restart vLLM with tool support
- Best possible results
- Production use

---

## ğŸ“ Complete File List

### Sliding Window (Recommended):
- âœ… `run_pde_discovery_simple_v04_sliding.py` - **Simple version (no vLLM restart)**
- âœ… `RUN_SIMPLE_V04_SLIDING.sh` - **Wrapper script**
- âœ… `run_pde_discovery_autogen_v04_sliding.py` - Tool calling version (needs vLLM restart)
- âœ… `RUN_AUTOGEN_V04_SLIDING.sh` - Wrapper for tool calling

### Agent Reset (Original Fix):
- âš ï¸ `run_pde_discovery_simple_v04_fixed.py` - Simple version with reset
- âš ï¸ `RUN_SIMPLE_V04_FIXED.sh` - Wrapper script

### Documentation:
- âœ… `CONTEXT_MANAGEMENT_COMPARISON.md` - This file
- âœ… `CONTEXT_FIX_STATUS.md` - Original reset fix documentation
- âœ… `VLLM_TOOL_CALLING_FIX.md` - Tool calling setup guide

---

## ğŸ‰ Summary

**The sliding window approach is superior to agent reset because:**

1. **Learning Preservation:** Agent remembers recent context
2. **Continuity:** Smooth progression through discovery
3. **Better Results:** Leverages recent learning
4. **Same Simplicity:** No added complexity for users
5. **Proven Pattern:** Standard approach in conversation AI

**Quick Start (Recommended):**
```bash
./RUN_SIMPLE_V04_SLIDING.sh
```

This will run 8000 iterations with:
- Sliding window context (20 messages)
- Trimming every 10 iterations
- Preserved learning throughout
- No context overflow
- Works with current vLLM

**Expect:** RÂ² â‰¥ 0.95 after 5000-8000 iterations
